#### ä½¿ç”¨å¼€æºé¡¹ç›®tensorflow/nmt


```python
# ä¸‹è½½é¡¹ç›®
!git clone https://github.com/tensorflow/nmt/
!ls
```

    fatal: destination path 'nmt' already exists and is not an empty directory.
    README.md             [1m[36mnmt[m[m                   seq2seq-chatbot.ipynb


**æˆ‘ä»¬ä¸‹è½½å°é»„é¸¡è¯­æ–™ï¼Œå¹¶å¯¹å®ƒåšä¸€ä¸ªå¤„ç†ï¼Œä½¿å¾—å®ƒç¬¦åˆseq2seqæ¨¡å‹çš„è¾“å…¥æ ¼å¼**


```python
!wget https://github.com/candlewill/Dialog_Corpus/raw/master/xiaohuangji50w_nofenci.conv.zip
!unzip xiaohuangji50w_nofenci.conv.zip
```

    --2019-12-02 10:29:01--  https://github.com/candlewill/Dialog_Corpus/raw/master/xiaohuangji50w_nofenci.conv.zip
    æ­£åœ¨è§£æä¸»æœº github.com (github.com)... 192.30.253.112
    æ­£åœ¨è¿æ¥ github.com (github.com)|192.30.253.112|:443... å·²è¿æ¥ã€‚
    å·²å‘å‡º HTTP è¯·æ±‚ï¼Œæ­£åœ¨ç­‰å¾…å›åº”... 302 Found
    ä½ç½®ï¼šhttps://raw.githubusercontent.com/candlewill/Dialog_Corpus/master/xiaohuangji50w_nofenci.conv.zip [è·Ÿéšè‡³æ–°çš„ URL]
    --2019-12-02 10:29:02--  https://raw.githubusercontent.com/candlewill/Dialog_Corpus/master/xiaohuangji50w_nofenci.conv.zip
    æ­£åœ¨è§£æä¸»æœº raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.76.133
    æ­£åœ¨è¿æ¥ raw.githubusercontent.com (raw.githubusercontent.com)|151.101.76.133|:443... å·²è¿æ¥ã€‚
    å·²å‘å‡º HTTP è¯·æ±‚ï¼Œæ­£åœ¨ç­‰å¾…å›åº”... 200 OK
    é•¿åº¦ï¼š10232473 (9.8M) [application/zip]
    æ­£åœ¨ä¿å­˜è‡³: â€œxiaohuangji50w_nofenci.conv.zipâ€
    
    xiaohuangji50w_nofe 100%[===================>]   9.76M  5.80MB/s  ç”¨æ—¶ 1.7s      
    
    2019-12-02 10:29:05 (5.80 MB/s) - å·²ä¿å­˜ â€œxiaohuangji50w_nofenci.conv.zipâ€ [10232473/10232473])
    
    Archive:  xiaohuangji50w_nofenci.conv.zip
      inflating: xiaohuangji50w_nofenci.conv  



```python
# æŠŠç¬¬ä¸€è¡Œçš„Eå’Œæ¢è¡Œæ¢åçš„ç¬¬äºŒè¡Œæ¢æˆQ M
!perl -pi.bak -e 's/(E\s)/\1Q /g' xiaohuangji50w_nofenci.conv
# æŠŠç¬¬äºŒè¡Œçš„(Q, M)æ¢æˆ Q
!perl -pi.bak -e 's/(Q M)/Q/g' xiaohuangji50w_nofenci.conv
# ç¬¬ä¸‰è¡Œçš„Mæ¢æˆ A
!perl -pi.bak -e 's/(M )/A /g' xiaohuangji50w_nofenci.conv
```


```python
!head -30 xiaohuangji50w_nofenci.conv
```

    E
    Q å‘µå‘µ
    A æ˜¯ç‹è‹¥çŒ«çš„ã€‚
    E
    Q ä¸æ˜¯
    A é‚£æ˜¯ä»€ä¹ˆï¼Ÿ
    E
    Q æ€ä¹ˆäº†
    A æˆ‘å¾ˆéš¾è¿‡ï¼Œå®‰æ…°æˆ‘~
    E
    Q å¼€å¿ƒç‚¹å“ˆ,ä¸€åˆ‡éƒ½ä¼šå¥½èµ·æ¥
    A å—¯ ä¼šçš„
    E
    Q æˆ‘è¿˜å–œæ¬¢å¥¹,æ€ä¹ˆåŠ
    A æˆ‘å¸®ä½ å‘Šè¯‰å¥¹ï¼Ÿå‘çŸ­ä¿¡è¿˜æ˜¯æ‰“ç”µè¯ï¼Ÿ
    E
    Q çŸ­ä¿¡
    A å—¯å—¯ã€‚æˆ‘ä¹Ÿç›¸ä¿¡
    E
    Q ä½ çŸ¥é“è°ä¹ˆ
    A è‚¯å®šä¸æ˜¯æˆ‘ï¼Œæ˜¯é˜®å¾·åŸ¹
    E
    Q è®¸å…µæ˜¯è°
    A å´é™¢å››ç­å°å¸…å“¥
    E
    Q è¿™ä¹ˆå‡
    A ä¸‰é¹¿å¥¶ç²‰ä¹Ÿå‡ï¼Œä¸ä¸€æ ·çš„å–å•Š
    E
    Q è®¸å…µæ˜¯å‚»é€¼
    A è¢«ä½ å‘ç°äº†ã€‚



```python
text = open('xiaohuangji50w_nofenci.conv').read().split('E\n')
text[1]
```




    'Q å‘µå‘µ\nA æ˜¯ç‹è‹¥çŒ«çš„ã€‚\n'



**åˆ†è¯**


```python
import jieba
def split_conv(in_f, out_q, out_a):
    out_question = open(out_q, 'w')
    out_answer = open(out_a, 'w')
    text = open(in_f).read().split('E\n')
    for pair in text:
        # å¥å­é•¿åº¦å¤ªçŸ­çš„å¯¹è¯ï¼Œå°±è¿‡æ»¤æ‰ï¼Œè·³è¿‡
        if len(pair) <= 4:
            continue
        # åˆ‡åˆ†é—®é¢˜å’Œå›ç­”
        contents = pair.split('\n')
        out_question.write(' '.join(jieba.lcut(contents[0].strip('Q '))) + '\n')
        out_answer.write(' '.join(jieba.lcut(contents[1].strip('A '))) + '\n')
    out_question.close()
    out_answer.close()
```


```python
in_f = 'xiaohuangji50w_nofenci.conv'
out_q = 'question.file'
out_a = 'answer.file'
split_conv(in_f, out_q, out_a)
```

    Building prefix dict from the default dictionary ...
    Loading model from cache /var/folders/g_/rv2sg65j1_g2znz05v_snmth0000gn/T/jieba.cache
    Loading model cost 0.652 seconds.
    Prefix dict has been built succesfully.


**æŸ¥çœ‹questionçš„å‰10è¡Œ**


```python
!head -10 question.file
```

    å‘µå‘µ
    ä¸æ˜¯
    æ€ä¹ˆ äº†
    å¼€å¿ƒ ç‚¹å“ˆ , ä¸€åˆ‡ éƒ½ ä¼š å¥½ èµ·æ¥
    æˆ‘ è¿˜ å–œæ¬¢ å¥¹ , æ€ä¹ˆåŠ
    çŸ­ä¿¡
    ä½  çŸ¥é“ è° ä¹ˆ
    è®¸å…µ æ˜¯ è°
    è¿™ä¹ˆ å‡
    è®¸å…µ æ˜¯ å‚» é€¼


**æŸ¥çœ‹ç­”æ¡ˆçš„å‰10è¡Œ**


```python
!head -10 answer.file
```

    æ˜¯ ç‹è‹¥ çŒ« çš„ ã€‚
    é‚£ æ˜¯ ä»€ä¹ˆ ï¼Ÿ
    æˆ‘ å¾ˆ éš¾è¿‡ ï¼Œ å®‰æ…° æˆ‘ ~
    å—¯   ä¼š çš„
    æˆ‘ å¸® ä½  å‘Šè¯‰ å¥¹ ï¼Ÿ å‘çŸ­ä¿¡ è¿˜æ˜¯ æ‰“ç”µè¯ ï¼Ÿ
    å—¯ å—¯ ã€‚ æˆ‘ ä¹Ÿ ç›¸ä¿¡
    è‚¯å®š ä¸æ˜¯ æˆ‘ ï¼Œ æ˜¯ é˜®å¾·åŸ¹
    å´é™¢ å››ç­ å°å¸…å“¥
    ä¸‰é¹¿ å¥¶ç²‰ ä¹Ÿ å‡ ï¼Œ ä¸ ä¸€æ · çš„ å– å•Š
    è¢« ä½  å‘ç° äº† ã€‚


**æŸ¥çœ‹é—®é¢˜ä¸€å…±æœ‰å¤šå°‘è¡Œ**


```python
!wc -l question.file
```

      454131 question.file


**æŸ¥çœ‹ç­”æ¡ˆä¸€å…±æœ‰å¤šå°‘è¡Œ**


```python
!wc -l answer.file
```

      454131 answer.file



```python
import re
def get_vocab(in_f, out_f):
    vocab_dic = {}
    for line in open(in_f, encoding='utf-8'):
        words = line.strip().split(' ')
        for word in words:
            # ä¿ç•™æ±‰å­—å†…å®¹
            if not re.match(r'[\u4e00-\u9fa5]+', word):
                continue
            try:
                vocab_dic[word] += 1
            except:
                vocab_dic[word] = 1
    out = open(out_f, 'w', encoding='utf-8')
    out.write("<unk>\n<s>\n</s>\n")
    vocab = sorted(vocab_dic.items(), key=lambda x:x[1],reverse=True)
    
    for word in [x[0] for x in vocab[:800000]]:
        out.write(word)
        out.write('\n')
    out.close()
```

**åˆ‡åˆ†è®­ç»ƒï¼ŒéªŒè¯ï¼Œæµ‹è¯•é›†**


```python
!mkdir data
# å‰300000ä½œä¸ºè®­ç»ƒé›†
!head -300000 question.file > data/train.input
!head -300000 answer.file > data/train.output
# å80000ä½œä¸ºéªŒè¯é›†
!head -380000 question.file | tail -80000 > data/val.input
!head -380000 question.file | tail -80000 > data/val.output
# æœ€å750000ä½œä¸ºæµ‹è¯•é›†
!tail -75000 question.file > data/test.input
!tail -75000 answer.file > data/test.output
```


```python
in_file = 'question.file'
out_file = './data/vocab.input'
get_vocab(in_file, out_file)
```


```python
in_file = 'answer.file'
out_file = './data/vocab.output'
get_vocab(in_file, out_file)
```


```python
!mkdir data/nmt_attention_model
```

**å‚è€ƒ [nmtçš„è¶…å‚æ•°](https://luozhouyang.github.io/tensorflow_nmt_hparams/)**


```python
# !python3 -m nmt.nmt \  
#     --attention=scaled_luong \  # ä½¿ç”¨attention çš„æ–¹å¼
#     --src=input --tgt=output \  # æºçš„ååº§
#     --vocab_prefix=./data/vocab  \  # vocab çš„å‰ç¼€
#     --train_prefix=./data/train \  # è®­ç»ƒæ•°æ®çš„å‰ç¼€
#     --dev_prefix=./data/val  \  # éªŒè¯é›†çš„å‰ç¼€
#     --test_prefix=./data/test \  # è®­ç»ƒé›†çš„å‰ç¼€
#     --out_dir=/tmp/nmt_attention_model \  # è¾“å‡ºçš„æ–‡ä»¶å¤¹
#     --num_train_steps=12000 \  # è¿­ä»£çš„æ­¥æ•°
#     --steps_per_stats=1 \  # å¤šå°‘æ­¥è¾“å‡ºä¸€æ¬¡çŠ¶æ€
#     --num_layers=2 \  # æ¯ä¸ªcellæœ‰å¤šå°‘å±‚
#     --num_units=128 \  # æœ‰å¤šå°‘ä¸ªç¥ç»å…ƒ
#     --dropout=0.2 \  # dropoutçš„æ¯”ç‡
#     --metrics=bleu  # è¯„ä¼°æŒ‡æ ‡
```


```python
!python -m nmt.nmt.nmt \
    --attention=scaled_luong \
    --src=input --tgt=output \
    --vocab_prefix=./data/vocab  \
    --train_prefix=./data/train \
    --dev_prefix=./data/val  \
    --test_prefix=./data/test \
    --out_dir=/tmp/nmt_attention_model \
    --num_train_steps=12000 \
    --steps_per_stats=1 \
    --num_layers=2 \
    --num_units=128 \
    --dropout=0.2 \
    --metrics=bleu 
```

    /Users/shinkeika/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
      _np_qint8 = np.dtype([("qint8", np.int8, 1)])
    /Users/shinkeika/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
      _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
    /Users/shinkeika/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
      _np_qint16 = np.dtype([("qint16", np.int16, 1)])
    /Users/shinkeika/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
      _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
    /Users/shinkeika/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
      _np_qint32 = np.dtype([("qint32", np.int32, 1)])
    /Users/shinkeika/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
      np_resource = np.dtype([("resource", np.ubyte, 1)])
    
    WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
    For more information, please see:
      * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
      * https://github.com/tensorflow/addons
    If you depend on functionality not listed there, please file an issue.
    
    # Job id 0
    2019-12-02 12:01:14.867781: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
    # Devices visible to TensorFlow: [_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 12028887416896540267)]
    # Creating output directory /tmp/nmt_attention_model ...
    # Vocab file ./data/vocab.input exists
    # Vocab file ./data/vocab.output exists
      saving hparams to /tmp/nmt_attention_model/hparams
      saving hparams to /tmp/nmt_attention_model/best_bleu/hparams
      attention=scaled_luong
      attention_architecture=standard
      avg_ckpts=False
      batch_size=128
      beam_width=0
      best_bleu=0
      best_bleu_dir=/tmp/nmt_attention_model/best_bleu
      check_special_token=True
      colocate_gradients_with_ops=True
      coverage_penalty_weight=0.0
      decay_scheme=
      dev_prefix=./data/val
      dropout=0.2
      embed_prefix=None
      encoder_type=uni
      eos=</s>
      epoch_step=0
      forget_bias=1.0
      infer_batch_size=32
      infer_mode=greedy
      init_op=uniform
      init_weight=0.1
      language_model=False
      learning_rate=1.0
      length_penalty_weight=0.0
      log_device_placement=False
      max_gradient_norm=5.0
      max_train=0
      metrics=['bleu']
      num_buckets=5
      num_dec_emb_partitions=0
      num_decoder_layers=2
      num_decoder_residual_layers=0
      num_embeddings_partitions=0
      num_enc_emb_partitions=0
      num_encoder_layers=2
      num_encoder_residual_layers=0
      num_gpus=1
      num_inter_threads=0
      num_intra_threads=0
      num_keep_ckpts=5
      num_sampled_softmax=0
      num_train_steps=12000
      num_translations_per_input=1
      num_units=128
      optimizer=sgd
      out_dir=/tmp/nmt_attention_model
      output_attention=True
      override_loaded_hparams=False
      pass_hidden_state=True
      random_seed=None
      residual=False
      sampling_temperature=0.0
      share_vocab=False
      sos=<s>
      src=input
      src_embed_file=
      src_max_len=50
      src_max_len_infer=None
      src_vocab_file=./data/vocab.input
      src_vocab_size=56491
      steps_per_external_eval=None
      steps_per_stats=1
      subword_option=
      test_prefix=./data/test
      tgt=output
      tgt_embed_file=
      tgt_max_len=50
      tgt_max_len_infer=None
      tgt_vocab_file=./data/vocab.output
      tgt_vocab_size=50041
      time_major=True
      train_prefix=./data/train
      unit_type=lstm
      use_char_encode=False
      vocab_prefix=./data/vocab
      warmup_scheme=t2t
      warmup_steps=0
    WARNING:tensorflow:From /Users/shinkeika/learning/nlp_project/4-chatbot/tensorflow-seq2seq-chatbot/nmt/nmt/utils/iterator_utils.py:129: DatasetV1.shard (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use `dataset.apply(tf.data.experimental.filter_for_shard(...))`.
    WARNING:tensorflow:From /Users/shinkeika/learning/nlp_project/4-chatbot/tensorflow-seq2seq-chatbot/nmt/nmt/utils/iterator_utils.py:235: group_by_window (from tensorflow.contrib.data.python.ops.grouping) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use `tf.data.experimental.group_by_window(...)`.
    WARNING:tensorflow:From /Users/shinkeika/learning/nlp_project/4-chatbot/tensorflow-seq2seq-chatbot/nmt/nmt/utils/iterator_utils.py:228: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use tf.cast instead.
    WARNING:tensorflow:From /Users/shinkeika/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Colocations handled automatically by placer.
    # Creating train graph ...
    # Build a basic encoder
      num_layers = 2, num_residual_layers=0
      cell 0  LSTM, forget_bias=1WARNING:tensorflow:From /Users/shinkeika/learning/nlp_project/4-chatbot/tensorflow-seq2seq-chatbot/nmt/nmt/model_helper.py:402: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
    Instructions for updating:
    This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.
      DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
      cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
    WARNING:tensorflow:From /Users/shinkeika/learning/nlp_project/4-chatbot/tensorflow-seq2seq-chatbot/nmt/nmt/model_helper.py:508: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
    Instructions for updating:
    This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.
    WARNING:tensorflow:From /Users/shinkeika/learning/nlp_project/4-chatbot/tensorflow-seq2seq-chatbot/nmt/nmt/model.py:767: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
    Instructions for updating:
    Please use `keras.layers.RNN(cell)`, which is equivalent to this API
    WARNING:tensorflow:From /Users/shinkeika/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use tf.cast instead.
    WARNING:tensorflow:From /Users/shinkeika/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
    Instructions for updating:
    Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
    WARNING:tensorflow:From /Users/shinkeika/learning/nlp_project/4-chatbot/tensorflow-seq2seq-chatbot/nmt/nmt/model.py:445: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use tf.cast instead.
      cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
      cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
      learning_rate=1, warmup_steps=0, warmup_scheme=t2t
      decay_scheme=, start_decay_step=12000, decay_steps 0, decay_factor 1
    # Trainable variables
    Format: <name>, <shape>, <(soft) device placement>
      embeddings/encoder/embedding_encoder:0, (56491, 128), /device:CPU:0
      embeddings/decoder/embedding_decoder:0, (50041, 128), /device:CPU:0
      dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
      dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0
      dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
      dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0
      dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), 
      dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0
      dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0
      dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
      dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0
      dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
      dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0
      dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 50041), /device:GPU:0
    # Creating eval graph ...
    # Build a basic encoder
      num_layers = 2, num_residual_layers=0
      cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
      cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
      cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
      cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
    # Trainable variables
    Format: <name>, <shape>, <(soft) device placement>
      embeddings/encoder/embedding_encoder:0, (56491, 128), /device:CPU:0
      embeddings/decoder/embedding_decoder:0, (50041, 128), /device:CPU:0
      dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
      dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0
      dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
      dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0
      dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), 
      dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0
      dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0
      dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
      dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0
      dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
      dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0
      dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 50041), /device:GPU:0
    # Creating infer graph ...
    # Build a basic encoder
      num_layers = 2, num_residual_layers=0
      cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
      cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
      cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
      cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
      decoder: infer_mode=greedybeam_width=0, length_penalty=0.000000, coverage_penalty=0.000000
    # Trainable variables
    Format: <name>, <shape>, <(soft) device placement>
      embeddings/encoder/embedding_encoder:0, (56491, 128), /device:CPU:0
      embeddings/decoder/embedding_decoder:0, (50041, 128), /device:CPU:0
      dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
      dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0
      dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
      dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0
      dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), 
      dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0
      dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0
      dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
      dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0
      dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0
      dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0
      dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 50041), 
    # log_file=/tmp/nmt_attention_model/log_1575259278
      created train model with fresh parameters, time 0.28s
      created infer model with fresh parameters, time 0.14s
      # 14721
        src: å» ä½  ä¸ª çš„
        ref: å» ä½  ä¸ª çš„
        nmt: å°å§å§ å‹çˆ± å‹çˆ± å‹çˆ± æ„›å¾ æ„›å¾ ä¸€ç®­åŒé›• ä¸€ç®­åŒé›•
      created eval model with fresh parameters, time 0.14s


