{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用开源项目tensorflow/nmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'nmt' already exists and is not an empty directory.\n",
      "README.md             \u001b[1m\u001b[36mnmt\u001b[m\u001b[m                   seq2seq-chatbot.ipynb\n"
     ]
    }
   ],
   "source": [
    "# 下载项目\n",
    "!git clone https://github.com/tensorflow/nmt/\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**我们下载小黄鸡语料，并对它做一个处理，使得它符合seq2seq模型的输入格式**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-12-02 10:29:01--  https://github.com/candlewill/Dialog_Corpus/raw/master/xiaohuangji50w_nofenci.conv.zip\n",
      "正在解析主机 github.com (github.com)... 192.30.253.112\n",
      "正在连接 github.com (github.com)|192.30.253.112|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 302 Found\n",
      "位置：https://raw.githubusercontent.com/candlewill/Dialog_Corpus/master/xiaohuangji50w_nofenci.conv.zip [跟随至新的 URL]\n",
      "--2019-12-02 10:29:02--  https://raw.githubusercontent.com/candlewill/Dialog_Corpus/master/xiaohuangji50w_nofenci.conv.zip\n",
      "正在解析主机 raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.76.133\n",
      "正在连接 raw.githubusercontent.com (raw.githubusercontent.com)|151.101.76.133|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度：10232473 (9.8M) [application/zip]\n",
      "正在保存至: “xiaohuangji50w_nofenci.conv.zip”\n",
      "\n",
      "xiaohuangji50w_nofe 100%[===================>]   9.76M  5.80MB/s  用时 1.7s      \n",
      "\n",
      "2019-12-02 10:29:05 (5.80 MB/s) - 已保存 “xiaohuangji50w_nofenci.conv.zip” [10232473/10232473])\n",
      "\n",
      "Archive:  xiaohuangji50w_nofenci.conv.zip\n",
      "  inflating: xiaohuangji50w_nofenci.conv  \n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/candlewill/Dialog_Corpus/raw/master/xiaohuangji50w_nofenci.conv.zip\n",
    "!unzip xiaohuangji50w_nofenci.conv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把第一行的E和换行换后的第二行换成Q M\n",
    "!perl -pi.bak -e 's/(E\\s)/\\1Q /g' xiaohuangji50w_nofenci.conv\n",
    "# 把第二行的(Q, M)换成 Q\n",
    "!perl -pi.bak -e 's/(Q M)/Q/g' xiaohuangji50w_nofenci.conv\n",
    "# 第三行的M换成 A\n",
    "!perl -pi.bak -e 's/(M )/A /g' xiaohuangji50w_nofenci.conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E\r\n",
      "Q 呵呵\r\n",
      "A 是王若猫的。\r\n",
      "E\r\n",
      "Q 不是\r\n",
      "A 那是什么？\r\n",
      "E\r\n",
      "Q 怎么了\r\n",
      "A 我很难过，安慰我~\r\n",
      "E\r\n",
      "Q 开心点哈,一切都会好起来\r\n",
      "A 嗯 会的\r\n",
      "E\r\n",
      "Q 我还喜欢她,怎么办\r\n",
      "A 我帮你告诉她？发短信还是打电话？\r\n",
      "E\r\n",
      "Q 短信\r\n",
      "A 嗯嗯。我也相信\r\n",
      "E\r\n",
      "Q 你知道谁么\r\n",
      "A 肯定不是我，是阮德培\r\n",
      "E\r\n",
      "Q 许兵是谁\r\n",
      "A 吴院四班小帅哥\r\n",
      "E\r\n",
      "Q 这么假\r\n",
      "A 三鹿奶粉也假，不一样的卖啊\r\n",
      "E\r\n",
      "Q 许兵是傻逼\r\n",
      "A 被你发现了。\r\n"
     ]
    }
   ],
   "source": [
    "!head -30 xiaohuangji50w_nofenci.conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q 呵呵\\nA 是王若猫的。\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open('xiaohuangji50w_nofenci.conv').read().split('E\\n')\n",
    "text[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**分词**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "def split_conv(in_f, out_q, out_a):\n",
    "    out_question = open(out_q, 'w')\n",
    "    out_answer = open(out_a, 'w')\n",
    "    text = open(in_f).read().split('E\\n')\n",
    "    for pair in text:\n",
    "        # 句子长度太短的对话，就过滤掉，跳过\n",
    "        if len(pair) <= 4:\n",
    "            continue\n",
    "        # 切分问题和回答\n",
    "        contents = pair.split('\\n')\n",
    "        out_question.write(' '.join(jieba.lcut(contents[0].strip('Q '))) + '\\n')\n",
    "        out_answer.write(' '.join(jieba.lcut(contents[1].strip('A '))) + '\\n')\n",
    "    out_question.close()\n",
    "    out_answer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/g_/rv2sg65j1_g2znz05v_snmth0000gn/T/jieba.cache\n",
      "Loading model cost 0.652 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "in_f = 'xiaohuangji50w_nofenci.conv'\n",
    "out_q = 'question.file'\n",
    "out_a = 'answer.file'\n",
    "split_conv(in_f, out_q, out_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**查看question的前10行**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "呵呵\r\n",
      "不是\r\n",
      "怎么 了\r\n",
      "开心 点哈 , 一切 都 会 好 起来\r\n",
      "我 还 喜欢 她 , 怎么办\r\n",
      "短信\r\n",
      "你 知道 谁 么\r\n",
      "许兵 是 谁\r\n",
      "这么 假\r\n",
      "许兵 是 傻 逼\r\n"
     ]
    }
   ],
   "source": [
    "!head -10 question.file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**查看答案的前10行**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "是 王若 猫 的 。\r\n",
      "那 是 什么 ？\r\n",
      "我 很 难过 ， 安慰 我 ~\r\n",
      "嗯   会 的\r\n",
      "我 帮 你 告诉 她 ？ 发短信 还是 打电话 ？\r\n",
      "嗯 嗯 。 我 也 相信\r\n",
      "肯定 不是 我 ， 是 阮德培\r\n",
      "吴院 四班 小帅哥\r\n",
      "三鹿 奶粉 也 假 ， 不 一样 的 卖 啊\r\n",
      "被 你 发现 了 。\r\n"
     ]
    }
   ],
   "source": [
    "!head -10 answer.file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**查看问题一共有多少行**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  454131 question.file\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l question.file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**查看答案一共有多少行**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  454131 answer.file\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l answer.file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_vocab(in_f, out_f):\n",
    "    vocab_dic = {}\n",
    "    for line in open(in_f, encoding='utf-8'):\n",
    "        words = line.strip().split(' ')\n",
    "        for word in words:\n",
    "            # 保留汉字内容\n",
    "            if not re.match(r'[\\u4e00-\\u9fa5]+', word):\n",
    "                continue\n",
    "            try:\n",
    "                vocab_dic[word] += 1\n",
    "            except:\n",
    "                vocab_dic[word] = 1\n",
    "    out = open(out_f, 'w', encoding='utf-8')\n",
    "    out.write(\"<unk>\\n<s>\\n</s>\\n\")\n",
    "    vocab = sorted(vocab_dic.items(), key=lambda x:x[1],reverse=True)\n",
    "    \n",
    "    for word in [x[0] for x in vocab[:800000]]:\n",
    "        out.write(word)\n",
    "        out.write('\\n')\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**切分训练，验证，测试集**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "# 前300000作为训练集\n",
    "!head -300000 question.file > data/train.input\n",
    "!head -300000 answer.file > data/train.output\n",
    "# 后80000作为验证集\n",
    "!head -380000 question.file | tail -80000 > data/val.input\n",
    "!head -380000 question.file | tail -80000 > data/val.output\n",
    "# 最后750000作为测试集\n",
    "!tail -75000 question.file > data/test.input\n",
    "!tail -75000 answer.file > data/test.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = 'question.file'\n",
    "out_file = './data/vocab.input'\n",
    "get_vocab(in_file, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = 'answer.file'\n",
    "out_file = './data/vocab.output'\n",
    "get_vocab(in_file, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data/nmt_attention_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**参考 [nmt的超参数](https://luozhouyang.github.io/tensorflow_nmt_hparams/)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m nmt.nmt \\  \n",
    "#     --attention=scaled_luong \\  # 使用attention 的方式\n",
    "#     --src=input --tgt=output \\  # 源的后座\n",
    "#     --vocab_prefix=./data/vocab  \\  # vocab 的前缀\n",
    "#     --train_prefix=./data/train \\  # 训练数据的前缀\n",
    "#     --dev_prefix=./data/val  \\  # 验证集的前缀\n",
    "#     --test_prefix=./data/test \\  # 训练集的前缀\n",
    "#     --out_dir=/tmp/nmt_attention_model \\  # 输出的文件夹\n",
    "#     --num_train_steps=12000 \\  # 迭代的步数\n",
    "#     --steps_per_stats=1 \\  # 多少步输出一次状态\n",
    "#     --num_layers=2 \\  # 每个cell有多少层\n",
    "#     --num_units=128 \\  # 有多少个神经元\n",
    "#     --dropout=0.2 \\  # dropout的比率\n",
    "#     --metrics=bleu  # 评估指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shinkeika/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/shinkeika/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/shinkeika/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/shinkeika/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/shinkeika/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/shinkeika/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "# Job id 0\n",
      "2019-12-02 12:01:14.867781: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "# Devices visible to TensorFlow: [_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 12028887416896540267)]\n",
      "# Creating output directory /tmp/nmt_attention_model ...\n",
      "# Vocab file ./data/vocab.input exists\n",
      "# Vocab file ./data/vocab.output exists\n",
      "  saving hparams to /tmp/nmt_attention_model/hparams\n",
      "  saving hparams to /tmp/nmt_attention_model/best_bleu/hparams\n",
      "  attention=scaled_luong\n",
      "  attention_architecture=standard\n",
      "  avg_ckpts=False\n",
      "  batch_size=128\n",
      "  beam_width=0\n",
      "  best_bleu=0\n",
      "  best_bleu_dir=/tmp/nmt_attention_model/best_bleu\n",
      "  check_special_token=True\n",
      "  colocate_gradients_with_ops=True\n",
      "  coverage_penalty_weight=0.0\n",
      "  decay_scheme=\n",
      "  dev_prefix=./data/val\n",
      "  dropout=0.2\n",
      "  embed_prefix=None\n",
      "  encoder_type=uni\n",
      "  eos=</s>\n",
      "  epoch_step=0\n",
      "  forget_bias=1.0\n",
      "  infer_batch_size=32\n",
      "  infer_mode=greedy\n",
      "  init_op=uniform\n",
      "  init_weight=0.1\n",
      "  language_model=False\n",
      "  learning_rate=1.0\n",
      "  length_penalty_weight=0.0\n",
      "  log_device_placement=False\n",
      "  max_gradient_norm=5.0\n",
      "  max_train=0\n",
      "  metrics=['bleu']\n",
      "  num_buckets=5\n",
      "  num_dec_emb_partitions=0\n",
      "  num_decoder_layers=2\n",
      "  num_decoder_residual_layers=0\n",
      "  num_embeddings_partitions=0\n",
      "  num_enc_emb_partitions=0\n",
      "  num_encoder_layers=2\n",
      "  num_encoder_residual_layers=0\n",
      "  num_gpus=1\n",
      "  num_inter_threads=0\n",
      "  num_intra_threads=0\n",
      "  num_keep_ckpts=5\n",
      "  num_sampled_softmax=0\n",
      "  num_train_steps=12000\n",
      "  num_translations_per_input=1\n",
      "  num_units=128\n",
      "  optimizer=sgd\n",
      "  out_dir=/tmp/nmt_attention_model\n",
      "  output_attention=True\n",
      "  override_loaded_hparams=False\n",
      "  pass_hidden_state=True\n",
      "  random_seed=None\n",
      "  residual=False\n",
      "  sampling_temperature=0.0\n",
      "  share_vocab=False\n",
      "  sos=<s>\n",
      "  src=input\n",
      "  src_embed_file=\n",
      "  src_max_len=50\n",
      "  src_max_len_infer=None\n",
      "  src_vocab_file=./data/vocab.input\n",
      "  src_vocab_size=56491\n",
      "  steps_per_external_eval=None\n",
      "  steps_per_stats=1\n",
      "  subword_option=\n",
      "  test_prefix=./data/test\n",
      "  tgt=output\n",
      "  tgt_embed_file=\n",
      "  tgt_max_len=50\n",
      "  tgt_max_len_infer=None\n",
      "  tgt_vocab_file=./data/vocab.output\n",
      "  tgt_vocab_size=50041\n",
      "  time_major=True\n",
      "  train_prefix=./data/train\n",
      "  unit_type=lstm\n",
      "  use_char_encode=False\n",
      "  vocab_prefix=./data/vocab\n",
      "  warmup_scheme=t2t\n",
      "  warmup_steps=0\n",
      "WARNING:tensorflow:From /Users/shinkeika/learning/nlp_project/4-chatbot/tensorflow-seq2seq-chatbot/nmt/nmt/utils/iterator_utils.py:129: DatasetV1.shard (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `dataset.apply(tf.data.experimental.filter_for_shard(...))`.\n",
      "WARNING:tensorflow:From /Users/shinkeika/learning/nlp_project/4-chatbot/tensorflow-seq2seq-chatbot/nmt/nmt/utils/iterator_utils.py:235: group_by_window (from tensorflow.contrib.data.python.ops.grouping) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.group_by_window(...)`.\n",
      "WARNING:tensorflow:From /Users/shinkeika/learning/nlp_project/4-chatbot/tensorflow-seq2seq-chatbot/nmt/nmt/utils/iterator_utils.py:228: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/shinkeika/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "# Creating train graph ...\n",
      "# Build a basic encoder\n",
      "  num_layers = 2, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1WARNING:tensorflow:From /Users/shinkeika/learning/nlp_project/4-chatbot/tensorflow-seq2seq-chatbot/nmt/nmt/model_helper.py:402: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
      "WARNING:tensorflow:From /Users/shinkeika/learning/nlp_project/4-chatbot/tensorflow-seq2seq-chatbot/nmt/nmt/model_helper.py:508: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /Users/shinkeika/learning/nlp_project/4-chatbot/tensorflow-seq2seq-chatbot/nmt/nmt/model.py:767: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/shinkeika/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/shinkeika/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/shinkeika/learning/nlp_project/4-chatbot/tensorflow-seq2seq-chatbot/nmt/nmt/model.py:445: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  learning_rate=1, warmup_steps=0, warmup_scheme=t2t\n",
      "  decay_scheme=, start_decay_step=12000, decay_steps 0, decay_factor 1\n",
      "# Trainable variables\n",
      "Format: <name>, <shape>, <(soft) device placement>\n",
      "  embeddings/encoder/embedding_encoder:0, (56491, 128), /device:CPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (50041, 128), /device:CPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), \n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 50041), /device:GPU:0\n",
      "# Creating eval graph ...\n",
      "# Build a basic encoder\n",
      "  num_layers = 2, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "# Trainable variables\n",
      "Format: <name>, <shape>, <(soft) device placement>\n",
      "  embeddings/encoder/embedding_encoder:0, (56491, 128), /device:CPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (50041, 128), /device:CPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), \n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 50041), /device:GPU:0\n",
      "# Creating infer graph ...\n",
      "# Build a basic encoder\n",
      "  num_layers = 2, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  decoder: infer_mode=greedybeam_width=0, length_penalty=0.000000, coverage_penalty=0.000000\n",
      "# Trainable variables\n",
      "Format: <name>, <shape>, <(soft) device placement>\n",
      "  embeddings/encoder/embedding_encoder:0, (56491, 128), /device:CPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (50041, 128), /device:CPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), \n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 50041), \n",
      "# log_file=/tmp/nmt_attention_model/log_1575259278\n",
      "  created train model with fresh parameters, time 0.28s\n",
      "  created infer model with fresh parameters, time 0.14s\n",
      "  # 14721\n",
      "    src: 去 你 个 的\n",
      "    ref: 去 你 个 的\n",
      "    nmt: 小姐姐 友爱 友爱 友爱 愛徐 愛徐 一箭双雕 一箭双雕\n",
      "  created eval model with fresh parameters, time 0.14s\n"
     ]
    }
   ],
   "source": [
    "!python -m nmt.nmt.nmt \\\n",
    "    --attention=scaled_luong \\\n",
    "    --src=input --tgt=output \\\n",
    "    --vocab_prefix=./data/vocab  \\\n",
    "    --train_prefix=./data/train \\\n",
    "    --dev_prefix=./data/val  \\\n",
    "    --test_prefix=./data/test \\\n",
    "    --out_dir=/tmp/nmt_attention_model \\\n",
    "    --num_train_steps=12000 \\\n",
    "    --steps_per_stats=1 \\\n",
    "    --num_layers=2 \\\n",
    "    --num_units=128 \\\n",
    "    --dropout=0.2 \\\n",
    "    --metrics=bleu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
